{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#library import\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import utils\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras import models, layers, optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Add, concatenate, Input\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers.convolutional import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    print('[*]Set seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model 평가함수 작성\n",
    "recall + precision 을 이용하여 f1 score 계산\n",
    "model Callback 함수에서 사용\n",
    "'''\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 수, batch_size를 이용하여 step 계산\n",
    "'''\n",
    "def get_steps(num_samples, batch_size):\n",
    "    if (num_samples % batch_size) > 0:\n",
    "        return (num_samples // batch_size) + 1\n",
    "    else:\n",
    "        return num_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch 종료 시 마다 수행\n",
    "'''\n",
    "def get_callback(model_path, lr, monitor, patient=10, warmup_epoch=5, min_lr=0.00001):\n",
    "    \n",
    "    monitor = 'val_acc' if monitor is 'val_acc' else 'val_f1_m'\n",
    "    direction = 'max' if monitor in ['val_f1_m', 'val_acc'] else 'min'\n",
    "    \n",
    "    callbacks = [\n",
    "        #조기 종료\n",
    "        EarlyStopping(monitor=monitor,\n",
    "                      patience=patient,\n",
    "                      mode=direction,\n",
    "                      verbose=1),\n",
    "        #모델 저장\n",
    "        ModelCheckpoint(filepath=model_path,\n",
    "                        monitor=monitor,\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode=direction),\n",
    "        #lr decay\n",
    "        ReduceLROnPlateau(monitor = monitor,\n",
    "                          factor = 0.5,\n",
    "                          patience = patient / 4,\n",
    "                          min_lr=min_lr,\n",
    "                          verbose=1,\n",
    "                          mode=direction,\n",
    "                          warmup_epoch=warmup_epoch)\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file and return binary data\n",
    "'''\n",
    "read file -> binary values\n",
    "'''\n",
    "def getBinaryData(filename):\n",
    "\tbinaryValues = []\n",
    "\tfile = open(filename, \"rb\")\n",
    "\tdata = file.read(1)\n",
    "\twhile data != b\"\":\n",
    "\t\ttry:\n",
    "\t\t\tbinaryValues.append(ord(data))\n",
    "\n",
    "\t\texcept TypeError:\n",
    "\t\t\tpass\n",
    "\n",
    "\t\tdata = file.read(1)\n",
    "\n",
    "\treturn binaryValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#byte frequency\n",
    "def balanceByte(filename, table):\n",
    "    \n",
    "    fn = filename.split(\"/\")[-1]\n",
    "    zero = float(table[0])/4096.0\n",
    "    low = float(sum(table[1:31]))/4096.0\n",
    "    ascii = float(sum(table[32:127]))/4096.0\n",
    "    high = float(sum(table[128:254]))/4096.0\n",
    "    ff = float(table[255])/4096.0\n",
    "    return [zero, low, ascii, high, ff]\n",
    "\n",
    "def get_frequency(filename):\n",
    "    table = [0] * 256\n",
    "    data = open(filename, 'rb')\n",
    "    buff = data.read(2 ** 20)\n",
    "    while buff:\n",
    "        for c in buff:\n",
    "            table[c] += 1\n",
    "        buff = data.read(2 ** 20)\n",
    "    data.close()\n",
    "    table.extend(balanceByte(filename, table))\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path(path_dir):\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "        print('[*]Make dir: {}'.format(path_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(raw_file, data_dir):\n",
    "    if os.path.exists(data_dir):\n",
    "        return\n",
    "    os.makedirs(data_dir)\n",
    "    print('[*]Make dir: {}'.format(data_dir))\n",
    "    chunk_size = 4096\n",
    "    idx = 0\n",
    "    print('[*]Make bin file')\n",
    "    with open(raw_file, \"rb\") as f:\n",
    "        chunk = f.read(chunk_size)\n",
    "        while chunk:\n",
    "            with open(os.path.join(data_dir, str(idx)), \"wb\") as chunk_file:\n",
    "                chunk_file.write(chunk)\n",
    "            idx += 1\n",
    "            chunk = f.read(chunk_size)\n",
    "            print('>>{0:<10}'.format(idx), end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_byte_cols():\n",
    "    byte_cols = []\n",
    "    for col in range(0, 256):\n",
    "        byte_cols.append(col)\n",
    "    for col in ['0x00', 'low', 'ascii', 'high','0xff']:\n",
    "        byte_cols.append(col)\n",
    "    return byte_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frequency(df, data_dir, byte_cols):\n",
    "    #add frequency col\n",
    "    for col in byte_cols:\n",
    "        df[col] = None\n",
    "    rs = []\n",
    "    for idx in range(0, len(df)):\n",
    "        table = get_frequency(os.path.join(data_dir, df.iloc[idx]['file']))\n",
    "        series = dict(zip(byte_cols, table))\n",
    "        rs.append(series)\n",
    "    df.loc[:, byte_cols] = pd.DataFrame(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bindata(df, data_dir):\n",
    "    df['data'] = df['file'].map(lambda x: getBinaryData(os.path.join(data_dir, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_type_big(df):\n",
    "    df['type_big'] = df['type'].str.split('-').map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add binary data\n",
    "standard scaling\n",
    "'''\n",
    "def add_col(df, data_dir):\n",
    "    df = df.copy()\n",
    "    df['file'] = df['file'].astype(str)\n",
    "    #add data column\n",
    "    add_frequency(df, data_dir, byte_cols=get_byte_cols())\n",
    "    add_bindata(df, data_dir)\n",
    "    add_type_big(df)\n",
    "    print('[*]Add col')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(df, scaler):\n",
    "    df = df.copy()\n",
    "    #standard scaler\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != 'object':\n",
    "            scaler.fit(df[[col]])\n",
    "            df[col] = scaler.transform(df[[col]])\n",
    "    print('[*]Apply Scaler')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1D(filter_num,filter_size, dim, num_classes, \n",
    "                  activation='relu', maxpool_size=2):\n",
    "    #cnn input\n",
    "    cnn_input = Input(shape=(dim, 1))\n",
    "    #cnn layer\n",
    "    cnn_layer = Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num)(cnn_input)\n",
    "    cnn_layer = BatchNormalization()(cnn_layer)\n",
    "    cnn_layer = Activation(activation)(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=(maxpool_size))(cnn_layer)\n",
    "    \n",
    "    cnn_layer = Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num)(cnn_layer)\n",
    "    cnn_layer = BatchNormalization()(cnn_layer)\n",
    "    cnn_layer = Activation(activation)(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=(maxpool_size))(cnn_layer)\n",
    "    \n",
    "    cnn_layer = Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num)(cnn_layer)\n",
    "    cnn_layer = BatchNormalization()(cnn_layer)\n",
    "    cnn_layer = Activation(activation)(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=(maxpool_size))(cnn_layer)\n",
    "    cnn_layer = Flatten()(cnn_layer)\n",
    "    \n",
    "    #byte input\n",
    "    byte_input = Input(shape=(261,1), name='byte_input')\n",
    "    #byte layer\n",
    "    byte_layer = Flatten()(byte_input)\n",
    "    \n",
    "    merge_layer = concatenate([cnn_layer, byte_layer])\n",
    "    \n",
    "    merge_layer = Dense(256)(merge_layer)\n",
    "    merge_layer = BatchNormalization()(merge_layer)\n",
    "    merge_layer = Activation(activation)(merge_layer)\n",
    "    \n",
    "    merge_layer = Dense(64)(merge_layer)\n",
    "    merge_layer = BatchNormalization()(merge_layer)\n",
    "    merge_layer = Activation(activation)(merge_layer)\n",
    "    \n",
    "    merge_layer = Dense(num_classes)(merge_layer)\n",
    "    merge_layer = Activation('softmax')(merge_layer)\n",
    "    final_model = Model(inputs=[cnn_input, byte_input],\n",
    "                        outputs=merge_layer)\n",
    "    \n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data unbalanced -> remove\n",
    "'''\n",
    "def remove_oversample(df, target):\n",
    "    df = df.copy()\n",
    "    min_val = df[target].value_counts().min()\n",
    "    drop_indexes = []\n",
    "    vals = df['type_big'].unique()\n",
    "    for val in vals:\n",
    "        indexes = df[df['type_big']==val].index\n",
    "        if len(indexes) > min_val:\n",
    "            df = df.drop(indexes[min_val:])\n",
    "            drop_indexes.extend(indexes[min_val:])\n",
    "        print('[*]Drop {}: {}->{}'.format(target,\n",
    "                                          len(indexes),\n",
    "                                          min_val))\n",
    "    return df, drop_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "target -> label encoding\n",
    "'''\n",
    "def add_label(df, target):\n",
    "    df = df.copy()\n",
    "    target_label = '{}_label'.format(target)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[target])\n",
    "    df[target_label] = le.transform(df[target])\n",
    "    print('[*]Add col: {}'.format(target_label))\n",
    "    return df, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split x, y by kfold index\n",
    "'''\n",
    "def get_train_data(df, cnn_col, byte_cols, target, num_class, index):\n",
    "    array_dim = len(index)\n",
    "    byte_dim = len(byte_cols)\n",
    "    data_dim =len(df['data'][0])\n",
    "    X_train = np.array([df[cnn_col].iloc[index]])\n",
    "    X_train = X_train.reshape((array_dim, data_dim, -1))\n",
    "    X_train_byte = np.array([df[byte_cols].iloc[index,:].values])\n",
    "    X_train_byte = X_train_byte.reshape((array_dim, len(byte_cols), -1))\n",
    "    \n",
    "    Y_train = np.array([df[target].iloc[index]])\n",
    "    Y_train = utils.to_categorical(Y_train,\n",
    "                                   num_classes=num_class)\n",
    "    Y_train = Y_train.reshape((array_dim, num_class))\n",
    "    return X_train, X_train_byte, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, opt, skf, train_fold_step,\n",
    "          byte_cols, cnn_col, target, model_dir, monitor):\n",
    "    #check col\n",
    "    if cnn_col not in df.columns:\n",
    "        print('[*]{} is not in colummns'.format(cnn_col))\n",
    "    for col in byte_cols:\n",
    "        if col not in df.columns:\n",
    "            print('[*]{} is not in colummns'.format(col))\n",
    "            \n",
    "    histories = []\n",
    "        \n",
    "    for idx1, filter_num in enumerate(params['test_filter_nums']):\n",
    "        for idx2, filter_size in enumerate(params['test_filter_sizes']):\n",
    "            for fold_step, (train_index, valid_index) in enumerate(skf.split(df['type_big'], df['type_big'])):\n",
    "                print('=================filter_num: {} / filter_size: {} / fold: {}================='.format(filter_num, \n",
    "                                                                                                             filter_size,\n",
    "                                                                                                             fold_step))\n",
    "                #set model name\n",
    "                #save best model of kfold models\n",
    "                model_name = params['model_name_format'].format(filter_num, filter_size, fold_step)  # save model path\n",
    "                model_path = os.path.join(model_dir, model_name)\n",
    "                #num class\n",
    "                num_class = len(df[target].unique())\n",
    "                #cnn dim\n",
    "                cnn_dim = len(df[cnn_col][0])\n",
    "                #set train\n",
    "                X_train, X_train_byte, Y_train = get_train_data(df, cnn_col, byte_cols,\n",
    "                                                                target, num_class, train_index)\n",
    "                #set test\n",
    "                X_test, X_test_byte, Y_test = get_train_data(df, cnn_col, byte_cols,\n",
    "                                                             target, num_class, valid_index)\n",
    "                #set model\n",
    "                print(cnn_dim, num_class)\n",
    "                model = CNN1D(filter_num=filter_num, filter_size=filter_size,\n",
    "                              dim=cnn_dim, num_classes=num_class)\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=opt,\n",
    "                              metrics=['accuracy', f1_m])\n",
    "                \n",
    "                #train\n",
    "                history = model.fit([X_train,X_train_byte], Y_train, \n",
    "                                    batch_size=params['batch_size'],\n",
    "                                    epochs=params['epochs'],\n",
    "                                    validation_data=([X_test, X_test_byte], Y_test),\n",
    "                                    shuffle=True,\n",
    "                                    callbacks=get_callback(model_path, params['lr'], monitor),\n",
    "                                    verbose=1)\n",
    "                histories.append(history)\n",
    "\n",
    "                #for train one fold. optional.\n",
    "                if fold_step == train_fold_step-1:\n",
    "                    break\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get history dictionary to dataframe\n",
    "def histories_to_df(histories, target_col, train_fold_step):\n",
    "    history_df = pd.DataFrame()\n",
    "    idx = 0\n",
    "    for idx1, filter_num in enumerate(params['test_filter_nums']):\n",
    "        for idx2, filter_size in enumerate(params['test_filter_sizes']):\n",
    "            for fold in range(0, train_fold_step):\n",
    "                history = histories[idx].history\n",
    "                idx += 1\n",
    "                max_val_idx = history[target_col].index(max(history[target_col]))\n",
    "                temp = {}\n",
    "                for key in history.keys():\n",
    "                    temp[key] = history[key][max_val_idx]\n",
    "                history_df['num_{}_size_{}_fold_{}'.format(filter_num, filter_size, fold)] = pd.Series(temp)\n",
    "    return history_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(model, df):\n",
    "    X_test = X_test_byte = Y_test = None\n",
    "    #get test index from k fold\n",
    "    for fold_step, (train_index, valid_index) in enumerate(skf.split(df['type_big_label'], df['type_big_label'])):\n",
    "        X_test, X_test_byte, Y_test = get_train_data(df,\n",
    "                                                 cnn_col='data',\n",
    "                                                 byte_cols=get_byte_cols(),\n",
    "                                                 target=target_label, \n",
    "                                                 num_class=len(df[target_label].unique()),\n",
    "                                                 index=valid_index)\n",
    "        break\n",
    "    #get y_true\n",
    "    y_true = Y_test\n",
    "    #get y_pred\n",
    "    y_pred = model.predict([X_test, X_test_byte])\n",
    "    # get class name\n",
    "    classes = df[['type_big', 'type_big_label']].head(20)\n",
    "    classes = classes.groupby(['type_big', 'type_big_label'],as_index=False).size()\n",
    "    classes = classes.sort_values().index.levels[0].values\n",
    "    plot_confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1), classes, title='CM')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Set seed: 2\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "input_dir = 'input'\n",
    "#model save\n",
    "model_dir = 'models'\n",
    "#raw & meta csv\n",
    "raw_file = os.path.join(input_dir, 'raw_data.raw')\n",
    "csv_file = os.path.join(input_dir, 'part1.csv')\n",
    "#binary data folder\n",
    "bin_data_dir = os.path.join(input_dir, 'data_bin')\n",
    "df_fix_path = os.path.join(input_dir, 'data_fix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(raw_file):\n",
    "    print(\"[*]raw file isn's exists({})\".format(raw_file))\n",
    "if not os.path.exists(csv_file):\n",
    "    print(\"[*]csv file isn's exists({})\".format(csv_file))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Make dir: input/data_bin\n",
      "[*]Make bin file\n",
      ">>26400     \r"
     ]
    }
   ],
   "source": [
    "make_path(model_dir)\n",
    "make_data(raw_file, bin_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Add col\n",
      "[*]Apply Scaler\n",
      "[*]Add col: type_big_label\n",
      "[*]Drop type_big: 8596->5279\n",
      "[*]Drop type_big: 6764->5279\n",
      "[*]Drop type_big: 5761->5279\n",
      "[*]Drop type_big: 5279->5279\n"
     ]
    }
   ],
   "source": [
    "target = 'type_big'#target or target_big\n",
    "df_fix = None\n",
    "remove_sample = True\n",
    "if os.path.exists(df_fix_path):\n",
    "    df_fix = pd.read_csv(df_fix_path)\n",
    "    target_label = '{}_label'.format(target)\n",
    "else:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.drop(['offset'], axis=1)\n",
    "    df.columns = ['file', 'type']\n",
    "    df_fix = add_col(df, bin_data_dir)\n",
    "    df_fix = apply_scaler(df_fix, scaler=StandardScaler())\n",
    "    df_fix, target_label = add_label(df_fix, target)\n",
    "    df_fix.to_csv(df_fix_path, index=False)\n",
    "\n",
    "if remove_sample:\n",
    "    df_fix, _ = remove_oversample(df_fix, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video    5279\n",
       "enc      5279\n",
       "comp     5279\n",
       "image    5279\n",
       "Name: type_big, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix['type_big'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>type</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>0x00</th>\n",
       "      <th>low</th>\n",
       "      <th>ascii</th>\n",
       "      <th>high</th>\n",
       "      <th>0xff</th>\n",
       "      <th>data</th>\n",
       "      <th>type_big</th>\n",
       "      <th>type_big_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>enc-rsa</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00390625</td>\n",
       "      <td>0.11499</td>\n",
       "      <td>0.36084</td>\n",
       "      <td>0.507568</td>\n",
       "      <td>0.00170898</td>\n",
       "      <td>[99, 106, 215, 74, 148, 32, 137, 4, 92, 11, 36...</td>\n",
       "      <td>enc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  file     type   0   1   2   3   4   5   6   7      ...       254 255  \\\n",
       "0    0  enc-rsa  16  10  19  13  22  21  21  16      ...        17   7   \n",
       "\n",
       "         0x00      low    ascii      high        0xff  \\\n",
       "0  0.00390625  0.11499  0.36084  0.507568  0.00170898   \n",
       "\n",
       "                                                data type_big type_big_label  \n",
       "0  [99, 106, 215, 74, 148, 32, 137, 4, 92, 11, 36...      enc              1  \n",
       "\n",
       "[1 rows x 266 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'fold_splits': 10,\n",
    "          'train_fold_num': 1,#1 ~ fold_splits\n",
    "          'batch_size': 64,\n",
    "          'lr': 0.00001,\n",
    "          'epochs': 20,\n",
    "          'test_filter_nums': [4, 8],\n",
    "          'test_filter_sizes': [3, 4],\n",
    "          'model_name_format': 'model_conv1d_baseline_num_{}_size_{}_fold_{}',\n",
    "          'monitor': 'val_acc'#val_acc / val_f1_m\n",
    "         }\n",
    "opt = optimizers.Nadam(lr=params['lr'])  # optimizers\n",
    "skf = StratifiedKFold(n_splits=params['fold_splits'], random_state=SEED)  # StaratifiedKFold -> y label 밸런스 유지하며 k fold 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================filter_num: 4 / filter_size: 3 / fold: 0=================\n",
      "Train on 19004 samples, validate on 2112 samples\n",
      "Epoch 1/20\n",
      "19004/19004 [==============================] - 8s 439us/step - loss: 1.3577 - acc: 0.3735 - f1_m: 0.2681 - val_loss: 1.1872 - val_acc: 0.4811 - val_f1_m: 0.3828\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48106, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 2/20\n",
      "19004/19004 [==============================] - 5s 272us/step - loss: 1.0625 - acc: 0.5285 - f1_m: 0.4481 - val_loss: 1.0105 - val_acc: 0.5696 - val_f1_m: 0.4761\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48106 to 0.56960, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 3/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.9537 - acc: 0.5778 - f1_m: 0.5135 - val_loss: 0.9321 - val_acc: 0.5909 - val_f1_m: 0.5157\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.56960 to 0.59091, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 4/20\n",
      "19004/19004 [==============================] - 5s 277us/step - loss: 0.8923 - acc: 0.6016 - f1_m: 0.5411 - val_loss: 0.8880 - val_acc: 0.6037 - val_f1_m: 0.5326\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.59091 to 0.60369, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 5/20\n",
      "19004/19004 [==============================] - 5s 271us/step - loss: 0.8540 - acc: 0.6177 - f1_m: 0.5568 - val_loss: 0.8592 - val_acc: 0.6108 - val_f1_m: 0.5421\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.60369 to 0.61080, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 6/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.8208 - acc: 0.6299 - f1_m: 0.5697 - val_loss: 0.8384 - val_acc: 0.6165 - val_f1_m: 0.5485\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.61080 to 0.61648, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 7/20\n",
      "19004/19004 [==============================] - 5s 278us/step - loss: 0.7970 - acc: 0.6409 - f1_m: 0.5772 - val_loss: 0.8224 - val_acc: 0.6184 - val_f1_m: 0.5526\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.61648 to 0.61837, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 8/20\n",
      "19004/19004 [==============================] - 5s 275us/step - loss: 0.7768 - acc: 0.6486 - f1_m: 0.5879 - val_loss: 0.8086 - val_acc: 0.6179 - val_f1_m: 0.5611\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.61837\n",
      "Epoch 9/20\n",
      "19004/19004 [==============================] - 5s 271us/step - loss: 0.7606 - acc: 0.6595 - f1_m: 0.5957 - val_loss: 0.7988 - val_acc: 0.6231 - val_f1_m: 0.5568\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.61837 to 0.62311, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 10/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.7427 - acc: 0.6650 - f1_m: 0.6020 - val_loss: 0.7902 - val_acc: 0.6321 - val_f1_m: 0.5605\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.62311 to 0.63210, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 11/20\n",
      "19004/19004 [==============================] - 5s 269us/step - loss: 0.7296 - acc: 0.6744 - f1_m: 0.6108 - val_loss: 0.7825 - val_acc: 0.6340 - val_f1_m: 0.5607\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.63210 to 0.63400, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 12/20\n",
      "19004/19004 [==============================] - 5s 276us/step - loss: 0.7180 - acc: 0.6846 - f1_m: 0.6162 - val_loss: 0.7772 - val_acc: 0.6359 - val_f1_m: 0.5599\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.63400 to 0.63589, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 13/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.7043 - acc: 0.6909 - f1_m: 0.6264 - val_loss: 0.7718 - val_acc: 0.6364 - val_f1_m: 0.5593\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.63589 to 0.63636, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 14/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.6913 - acc: 0.6994 - f1_m: 0.6343 - val_loss: 0.7672 - val_acc: 0.6354 - val_f1_m: 0.5658\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.63636\n",
      "Epoch 15/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.6786 - acc: 0.7105 - f1_m: 0.6461 - val_loss: 0.7632 - val_acc: 0.6402 - val_f1_m: 0.5689\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.63636 to 0.64015, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 16/20\n",
      "19004/19004 [==============================] - 5s 272us/step - loss: 0.6686 - acc: 0.7178 - f1_m: 0.6541 - val_loss: 0.7600 - val_acc: 0.6430 - val_f1_m: 0.5707\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.64015 to 0.64299, saving model to models/model_conv1d_baseline_num_4_size_3_fold_0\n",
      "Epoch 17/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.6603 - acc: 0.7220 - f1_m: 0.6616 - val_loss: 0.7574 - val_acc: 0.6430 - val_f1_m: 0.5751\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.64299\n",
      "Epoch 18/20\n",
      "19004/19004 [==============================] - 5s 277us/step - loss: 0.6506 - acc: 0.7300 - f1_m: 0.6692 - val_loss: 0.7560 - val_acc: 0.6430 - val_f1_m: 0.5735\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.64299\n",
      "Epoch 19/20\n",
      "19004/19004 [==============================] - 5s 272us/step - loss: 0.6381 - acc: 0.7384 - f1_m: 0.6779 - val_loss: 0.7533 - val_acc: 0.6387 - val_f1_m: 0.5797\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.64299\n",
      "Epoch 20/20\n",
      "19004/19004 [==============================] - 5s 278us/step - loss: 0.6292 - acc: 0.7445 - f1_m: 0.6872 - val_loss: 0.7517 - val_acc: 0.6420 - val_f1_m: 0.5813\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.64299\n",
      "=================filter_num: 4 / filter_size: 4 / fold: 0=================\n",
      "Train on 19004 samples, validate on 2112 samples\n",
      "Epoch 1/20\n",
      "19004/19004 [==============================] - 9s 448us/step - loss: 1.0556 - acc: 0.5289 - f1_m: 0.4507 - val_loss: 0.9182 - val_acc: 0.5928 - val_f1_m: 0.5205\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59280, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 2/20\n",
      "19004/19004 [==============================] - 5s 271us/step - loss: 0.8738 - acc: 0.6154 - f1_m: 0.5455 - val_loss: 0.8617 - val_acc: 0.6051 - val_f1_m: 0.5395\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59280 to 0.60511, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 3/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.8220 - acc: 0.6320 - f1_m: 0.5638 - val_loss: 0.8325 - val_acc: 0.6084 - val_f1_m: 0.5511\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60511 to 0.60843, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 4/20\n",
      "19004/19004 [==============================] - 5s 272us/step - loss: 0.7878 - acc: 0.6503 - f1_m: 0.5813 - val_loss: 0.8145 - val_acc: 0.6155 - val_f1_m: 0.5600\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.60843 to 0.61553, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 5/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.7666 - acc: 0.6610 - f1_m: 0.5920 - val_loss: 0.8011 - val_acc: 0.6203 - val_f1_m: 0.5622\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61553 to 0.62027, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 6/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.7446 - acc: 0.6714 - f1_m: 0.6025 - val_loss: 0.7919 - val_acc: 0.6222 - val_f1_m: 0.5641\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.62027 to 0.62216, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 7/20\n",
      "19004/19004 [==============================] - 5s 277us/step - loss: 0.7302 - acc: 0.6820 - f1_m: 0.6130 - val_loss: 0.7831 - val_acc: 0.6207 - val_f1_m: 0.5682\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.62216\n",
      "Epoch 8/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.7149 - acc: 0.6891 - f1_m: 0.6258 - val_loss: 0.7762 - val_acc: 0.6212 - val_f1_m: 0.5717\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.62216\n",
      "Epoch 9/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.7011 - acc: 0.7002 - f1_m: 0.6330 - val_loss: 0.7712 - val_acc: 0.6241 - val_f1_m: 0.5741\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.62216 to 0.62405, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19004/19004 [==============================] - 5s 272us/step - loss: 0.6892 - acc: 0.7069 - f1_m: 0.6461 - val_loss: 0.7668 - val_acc: 0.6278 - val_f1_m: 0.5733\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.62405 to 0.62784, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 11/20\n",
      "19004/19004 [==============================] - 5s 271us/step - loss: 0.6785 - acc: 0.7156 - f1_m: 0.6541 - val_loss: 0.7632 - val_acc: 0.6264 - val_f1_m: 0.5747\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62784\n",
      "Epoch 12/20\n",
      "19004/19004 [==============================] - 5s 271us/step - loss: 0.6634 - acc: 0.7257 - f1_m: 0.6653 - val_loss: 0.7602 - val_acc: 0.6264 - val_f1_m: 0.5752\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62784\n",
      "Epoch 13/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.6543 - acc: 0.7337 - f1_m: 0.6709 - val_loss: 0.7579 - val_acc: 0.6274 - val_f1_m: 0.5753\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62784\n",
      "Epoch 14/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.6420 - acc: 0.7429 - f1_m: 0.6800 - val_loss: 0.7562 - val_acc: 0.6278 - val_f1_m: 0.5800\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62784\n",
      "Epoch 15/20\n",
      "19004/19004 [==============================] - 5s 276us/step - loss: 0.6313 - acc: 0.7491 - f1_m: 0.6904 - val_loss: 0.7551 - val_acc: 0.6278 - val_f1_m: 0.5813\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62784\n",
      "Epoch 16/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.6197 - acc: 0.7570 - f1_m: 0.7055 - val_loss: 0.7538 - val_acc: 0.6312 - val_f1_m: 0.5858\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.62784 to 0.63116, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 17/20\n",
      "19004/19004 [==============================] - 5s 273us/step - loss: 0.6085 - acc: 0.7627 - f1_m: 0.7128 - val_loss: 0.7536 - val_acc: 0.6283 - val_f1_m: 0.5882\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.63116\n",
      "Epoch 18/20\n",
      "19004/19004 [==============================] - 5s 276us/step - loss: 0.6002 - acc: 0.7685 - f1_m: 0.7166 - val_loss: 0.7530 - val_acc: 0.6326 - val_f1_m: 0.5911\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.63116 to 0.63258, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 19/20\n",
      "19004/19004 [==============================] - 5s 274us/step - loss: 0.5884 - acc: 0.7769 - f1_m: 0.7272 - val_loss: 0.7532 - val_acc: 0.6368 - val_f1_m: 0.5912\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.63258 to 0.63684, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "Epoch 20/20\n",
      "19004/19004 [==============================] - 5s 276us/step - loss: 0.5775 - acc: 0.7806 - f1_m: 0.7377 - val_loss: 0.7537 - val_acc: 0.6402 - val_f1_m: 0.5914\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.63684 to 0.64015, saving model to models/model_conv1d_baseline_num_4_size_4_fold_0\n",
      "=================filter_num: 8 / filter_size: 3 / fold: 0=================\n",
      "Train on 19004 samples, validate on 2112 samples\n",
      "Epoch 1/20\n",
      "19004/19004 [==============================] - 9s 491us/step - loss: 1.0388 - acc: 0.5334 - f1_m: 0.4523 - val_loss: 0.9217 - val_acc: 0.5739 - val_f1_m: 0.5209\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.57386, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 2/20\n",
      "19004/19004 [==============================] - 6s 304us/step - loss: 0.8240 - acc: 0.6296 - f1_m: 0.5701 - val_loss: 0.8528 - val_acc: 0.5952 - val_f1_m: 0.5400\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.57386 to 0.59517, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 3/20\n",
      "19004/19004 [==============================] - 6s 312us/step - loss: 0.7672 - acc: 0.6571 - f1_m: 0.5992 - val_loss: 0.8257 - val_acc: 0.6061 - val_f1_m: 0.5469\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.59517 to 0.60606, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 4/20\n",
      "19004/19004 [==============================] - 6s 309us/step - loss: 0.7298 - acc: 0.6797 - f1_m: 0.6199 - val_loss: 0.8085 - val_acc: 0.6108 - val_f1_m: 0.5507\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.60606 to 0.61080, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 5/20\n",
      "19004/19004 [==============================] - 6s 304us/step - loss: 0.7011 - acc: 0.7002 - f1_m: 0.6399 - val_loss: 0.7981 - val_acc: 0.6103 - val_f1_m: 0.5551\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61080\n",
      "Epoch 6/20\n",
      "19004/19004 [==============================] - 6s 307us/step - loss: 0.6784 - acc: 0.7148 - f1_m: 0.6539 - val_loss: 0.7896 - val_acc: 0.6136 - val_f1_m: 0.5584\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.61080 to 0.61364, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 7/20\n",
      "19004/19004 [==============================] - 6s 309us/step - loss: 0.6534 - acc: 0.7323 - f1_m: 0.6726 - val_loss: 0.7843 - val_acc: 0.6179 - val_f1_m: 0.5630\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.61364 to 0.61790, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 8/20\n",
      "19004/19004 [==============================] - 6s 303us/step - loss: 0.6348 - acc: 0.7456 - f1_m: 0.6879 - val_loss: 0.7803 - val_acc: 0.6184 - val_f1_m: 0.5651\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.61790 to 0.61837, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 9/20\n",
      "19004/19004 [==============================] - 6s 309us/step - loss: 0.6136 - acc: 0.7593 - f1_m: 0.7051 - val_loss: 0.7788 - val_acc: 0.6184 - val_f1_m: 0.5714\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.61837\n",
      "Epoch 10/20\n",
      "19004/19004 [==============================] - 6s 305us/step - loss: 0.5942 - acc: 0.7748 - f1_m: 0.7218 - val_loss: 0.7771 - val_acc: 0.6245 - val_f1_m: 0.5727\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.61837 to 0.62453, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 11/20\n",
      "19004/19004 [==============================] - 6s 311us/step - loss: 0.5721 - acc: 0.7906 - f1_m: 0.7410 - val_loss: 0.7777 - val_acc: 0.6236 - val_f1_m: 0.5746\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62453\n",
      "Epoch 12/20\n",
      "19004/19004 [==============================] - 6s 305us/step - loss: 0.5566 - acc: 0.8029 - f1_m: 0.7539 - val_loss: 0.7774 - val_acc: 0.6245 - val_f1_m: 0.5777\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62453\n",
      "Epoch 13/20\n",
      "19004/19004 [==============================] - 6s 307us/step - loss: 0.5374 - acc: 0.8140 - f1_m: 0.7700 - val_loss: 0.7787 - val_acc: 0.6269 - val_f1_m: 0.5821\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.62453 to 0.62689, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 14/20\n",
      "19004/19004 [==============================] - 6s 306us/step - loss: 0.5159 - acc: 0.8316 - f1_m: 0.7908 - val_loss: 0.7804 - val_acc: 0.6274 - val_f1_m: 0.5856\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.62689 to 0.62737, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 15/20\n",
      "19004/19004 [==============================] - 6s 312us/step - loss: 0.4991 - acc: 0.8403 - f1_m: 0.8035 - val_loss: 0.7823 - val_acc: 0.6312 - val_f1_m: 0.5886\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.62737 to 0.63116, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 16/20\n",
      "19004/19004 [==============================] - 6s 303us/step - loss: 0.4781 - acc: 0.8580 - f1_m: 0.8196 - val_loss: 0.7848 - val_acc: 0.6326 - val_f1_m: 0.5919\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.63116 to 0.63258, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "Epoch 17/20\n",
      "19004/19004 [==============================] - 6s 304us/step - loss: 0.4625 - acc: 0.8654 - f1_m: 0.8306 - val_loss: 0.7873 - val_acc: 0.6307 - val_f1_m: 0.5955\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.63258\n",
      "Epoch 18/20\n",
      "19004/19004 [==============================] - 6s 312us/step - loss: 0.4443 - acc: 0.8762 - f1_m: 0.8409 - val_loss: 0.7915 - val_acc: 0.6297 - val_f1_m: 0.5969\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.63258\n",
      "Epoch 19/20\n",
      "19004/19004 [==============================] - 6s 306us/step - loss: 0.4264 - acc: 0.8900 - f1_m: 0.8573 - val_loss: 0.7936 - val_acc: 0.6316 - val_f1_m: 0.5994\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.63258\n",
      "Epoch 20/20\n",
      "19004/19004 [==============================] - 6s 311us/step - loss: 0.4090 - acc: 0.8985 - f1_m: 0.8689 - val_loss: 0.7974 - val_acc: 0.6330 - val_f1_m: 0.6010\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.63258 to 0.63305, saving model to models/model_conv1d_baseline_num_8_size_3_fold_0\n",
      "=================filter_num: 8 / filter_size: 4 / fold: 0=================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19004 samples, validate on 2112 samples\n",
      "Epoch 1/20\n",
      "19004/19004 [==============================] - 10s 507us/step - loss: 0.9928 - acc: 0.5511 - f1_m: 0.4663 - val_loss: 0.8567 - val_acc: 0.6089 - val_f1_m: 0.5396\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60890, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 2/20\n",
      "19004/19004 [==============================] - 6s 302us/step - loss: 0.8076 - acc: 0.6328 - f1_m: 0.5671 - val_loss: 0.8125 - val_acc: 0.6264 - val_f1_m: 0.5537\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.60890 to 0.62642, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 3/20\n",
      "19004/19004 [==============================] - 6s 307us/step - loss: 0.7557 - acc: 0.6607 - f1_m: 0.5919 - val_loss: 0.7923 - val_acc: 0.6288 - val_f1_m: 0.5606\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62642 to 0.62879, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 4/20\n",
      "19004/19004 [==============================] - 6s 307us/step - loss: 0.7236 - acc: 0.6816 - f1_m: 0.6110 - val_loss: 0.7790 - val_acc: 0.6335 - val_f1_m: 0.5661\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62879 to 0.63352, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 5/20\n",
      "19004/19004 [==============================] - 6s 309us/step - loss: 0.6954 - acc: 0.7011 - f1_m: 0.6275 - val_loss: 0.7706 - val_acc: 0.6392 - val_f1_m: 0.5691\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.63352 to 0.63920, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 6/20\n",
      "19004/19004 [==============================] - 6s 308us/step - loss: 0.6747 - acc: 0.7155 - f1_m: 0.6442 - val_loss: 0.7639 - val_acc: 0.6425 - val_f1_m: 0.5692\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.63920 to 0.64252, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 7/20\n",
      "19004/19004 [==============================] - 6s 305us/step - loss: 0.6496 - acc: 0.7328 - f1_m: 0.6631 - val_loss: 0.7603 - val_acc: 0.6392 - val_f1_m: 0.5768\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.64252\n",
      "Epoch 8/20\n",
      "19004/19004 [==============================] - 6s 311us/step - loss: 0.6314 - acc: 0.7511 - f1_m: 0.6808 - val_loss: 0.7572 - val_acc: 0.6392 - val_f1_m: 0.5843\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.64252\n",
      "Epoch 9/20\n",
      "19004/19004 [==============================] - 6s 313us/step - loss: 0.6107 - acc: 0.7649 - f1_m: 0.6968 - val_loss: 0.7558 - val_acc: 0.6359 - val_f1_m: 0.5896\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.64252\n",
      "Epoch 10/20\n",
      "19004/19004 [==============================] - 6s 303us/step - loss: 0.5907 - acc: 0.7793 - f1_m: 0.7169 - val_loss: 0.7550 - val_acc: 0.6345 - val_f1_m: 0.5954\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.64252\n",
      "Epoch 11/20\n",
      "19004/19004 [==============================] - 6s 308us/step - loss: 0.5694 - acc: 0.7948 - f1_m: 0.7336 - val_loss: 0.7544 - val_acc: 0.6406 - val_f1_m: 0.5968\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.64252\n",
      "Epoch 12/20\n",
      "19004/19004 [==============================] - 6s 308us/step - loss: 0.5502 - acc: 0.8107 - f1_m: 0.7559 - val_loss: 0.7552 - val_acc: 0.6387 - val_f1_m: 0.6008\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.64252\n",
      "Epoch 13/20\n",
      "19004/19004 [==============================] - 6s 311us/step - loss: 0.5295 - acc: 0.8212 - f1_m: 0.7719 - val_loss: 0.7556 - val_acc: 0.6411 - val_f1_m: 0.6054\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.64252\n",
      "Epoch 14/20\n",
      "19004/19004 [==============================] - 6s 309us/step - loss: 0.5130 - acc: 0.8328 - f1_m: 0.7871 - val_loss: 0.7581 - val_acc: 0.6425 - val_f1_m: 0.6059\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.64252\n",
      "Epoch 15/20\n",
      "19004/19004 [==============================] - 6s 307us/step - loss: 0.4947 - acc: 0.8436 - f1_m: 0.8023 - val_loss: 0.7603 - val_acc: 0.6430 - val_f1_m: 0.6046\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.64252 to 0.64299, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 16/20\n",
      "19004/19004 [==============================] - 6s 310us/step - loss: 0.4748 - acc: 0.8567 - f1_m: 0.8161 - val_loss: 0.7627 - val_acc: 0.6411 - val_f1_m: 0.6077\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.64299\n",
      "Epoch 17/20\n",
      "19004/19004 [==============================] - 6s 312us/step - loss: 0.4535 - acc: 0.8734 - f1_m: 0.8349 - val_loss: 0.7650 - val_acc: 0.6411 - val_f1_m: 0.6081\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.64299\n",
      "Epoch 18/20\n",
      "19004/19004 [==============================] - 6s 312us/step - loss: 0.4358 - acc: 0.8849 - f1_m: 0.8495 - val_loss: 0.7677 - val_acc: 0.6416 - val_f1_m: 0.6135\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.64299\n",
      "Epoch 19/20\n",
      "19004/19004 [==============================] - 6s 311us/step - loss: 0.4180 - acc: 0.8938 - f1_m: 0.8617 - val_loss: 0.7712 - val_acc: 0.6439 - val_f1_m: 0.6166\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.64299 to 0.64394, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n",
      "Epoch 20/20\n",
      "19004/19004 [==============================] - 6s 306us/step - loss: 0.4001 - acc: 0.9068 - f1_m: 0.8761 - val_loss: 0.7750 - val_acc: 0.6449 - val_f1_m: 0.6156\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.64394 to 0.64489, saving model to models/model_conv1d_baseline_num_8_size_4_fold_0\n"
     ]
    }
   ],
   "source": [
    "histories = train(df=df_fix,\n",
    "                  opt=opt,\n",
    "                  skf=skf,\n",
    "                  train_fold_step=params['train_fold_num'],\n",
    "                  byte_cols=get_byte_cols(),\n",
    "                  cnn_col='data',\n",
    "                  target=target_label,\n",
    "                  model_dir=model_dir,\n",
    "                  monitor=params['monitor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_m</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1_m</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_4_size_3_fold_0</th>\n",
       "      <td>0.717849</td>\n",
       "      <td>0.654095</td>\n",
       "      <td>0.668643</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.642992</td>\n",
       "      <td>0.570685</td>\n",
       "      <td>0.759989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_4_size_4_fold_0</th>\n",
       "      <td>0.780573</td>\n",
       "      <td>0.737683</td>\n",
       "      <td>0.577494</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.640152</td>\n",
       "      <td>0.591373</td>\n",
       "      <td>0.753711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_8_size_3_fold_0</th>\n",
       "      <td>0.898495</td>\n",
       "      <td>0.868851</td>\n",
       "      <td>0.409023</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.633049</td>\n",
       "      <td>0.601049</td>\n",
       "      <td>0.797368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_8_size_4_fold_0</th>\n",
       "      <td>0.906809</td>\n",
       "      <td>0.876118</td>\n",
       "      <td>0.400098</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.644886</td>\n",
       "      <td>0.615555</td>\n",
       "      <td>0.775025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          acc      f1_m      loss       lr   val_acc  \\\n",
       "num_4_size_3_fold_0  0.717849  0.654095  0.668643  0.00001  0.642992   \n",
       "num_4_size_4_fold_0  0.780573  0.737683  0.577494  0.00001  0.640152   \n",
       "num_8_size_3_fold_0  0.898495  0.868851  0.409023  0.00001  0.633049   \n",
       "num_8_size_4_fold_0  0.906809  0.876118  0.400098  0.00001  0.644886   \n",
       "\n",
       "                     val_f1_m  val_loss  \n",
       "num_4_size_3_fold_0  0.570685  0.759989  \n",
       "num_4_size_4_fold_0  0.591373  0.753711  \n",
       "num_8_size_3_fold_0  0.601049  0.797368  \n",
       "num_8_size_4_fold_0  0.615555  0.775025  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df = histories_to_df(histories,\n",
    "                             target_col=params['monitor'],\n",
    "                             train_fold_step=params['train_fold_num'])\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_m</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1_m</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_8_size_4_fold_0</th>\n",
       "      <td>0.906809</td>\n",
       "      <td>0.876118</td>\n",
       "      <td>0.400098</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.644886</td>\n",
       "      <td>0.615555</td>\n",
       "      <td>0.775025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          acc      f1_m      loss       lr   val_acc  \\\n",
       "num_8_size_4_fold_0  0.906809  0.876118  0.400098  0.00001  0.644886   \n",
       "\n",
       "                     val_f1_m  val_loss  \n",
       "num_8_size_4_fold_0  0.615555  0.775025  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc = history_df.loc[history_df[params['monitor']].idxmax()]\n",
    "best_num, best_size, best_fold = [int(x) for x in re.compile('([+\\d])').findall(best_acc.name)]\n",
    "pd.DataFrame(best_acc).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model/weight\n",
    "model = CNN1D(filter_num=num,\n",
    "              filter_size=size,\n",
    "              dim=len(df_fix[target_label].unique()),\n",
    "              num_classes=len(df_fix['data'][0]))\n",
    "\n",
    "model_path = params['model_name_format'].format(best_num,\n",
    "                                                best_size,\n",
    "                                                best_fold)\n",
    "weight_path = os.path.join(model_dir, model_path)\n",
    "model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[335 109   3  81]\n",
      " [ 33 287   5 203]\n",
      " [  6   7 497  18]\n",
      " [ 52 231   2 243]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "#get test index from train data with kfold\n",
    "plot_cm(model, df_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
