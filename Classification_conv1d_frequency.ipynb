{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#library import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import models, layers, optimizers\n",
    "from keras import utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Add, concatenate, Input\n",
    "from keras.layers.convolutional import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import cv2, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    print('[*]Set seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model 평가함수 작성\n",
    "recall + precision 을 이용하여 f1 score 계산\n",
    "model Callback 함수에서 사용\n",
    "'''\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 수, batch_size를 이용하여 step 계산\n",
    "'''\n",
    "def get_steps(num_samples, batch_size):\n",
    "    if (num_samples % batch_size) > 0:\n",
    "        return (num_samples // batch_size) + 1\n",
    "    else:\n",
    "        return num_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch 종료 시 마다 수행\n",
    "'''\n",
    "def get_callback(model_path, lr, mode='val_f1_m', patient=10, warmup_epoch=5, min_lr=0.00001):\n",
    "    \n",
    "    mode = 'acc' if mode is 'acc' else 'val_f1_m'\n",
    "    direction = 'max' if mode=='val_f1_m' else 'min'\n",
    "    \n",
    "    callbacks = [\n",
    "        #조기 종료\n",
    "        EarlyStopping(monitor='val_f1_m',\n",
    "                      patience=patient,\n",
    "                      mode=direction,\n",
    "                      verbose=1),\n",
    "        #모델 저장\n",
    "        ModelCheckpoint(filepath=model_path,\n",
    "                        monitor='val_f1_m',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode=direction),\n",
    "        #lr decay\n",
    "        ReduceLROnPlateau(monitor = 'val_f1_m',\n",
    "                          factor = 0.5,\n",
    "                          patience = patient / 4,\n",
    "                          min_lr=min_lr,\n",
    "                          verbose=1,\n",
    "                          mode=direction,\n",
    "                          warmup_epoch=warmup_epoch)\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file and return binary data\n",
    "'''\n",
    "read file -> binary values\n",
    "'''\n",
    "def getBinaryData(filename):\n",
    "\tbinaryValues = []\n",
    "\tfile = open(filename, \"rb\")\n",
    "\tdata = file.read(1)\n",
    "\twhile data != b\"\":\n",
    "\t\ttry:\n",
    "\t\t\tbinaryValues.append(ord(data))\n",
    "\n",
    "\t\texcept TypeError:\n",
    "\t\t\tpass\n",
    "\n",
    "\t\tdata = file.read(1)\n",
    "\n",
    "\treturn binaryValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#byte frequency\n",
    "def balanceByte(filename, table):\n",
    "    \n",
    "    fn = filename.split(\"/\")[-1]\n",
    "    zero = float(table[0])/4096.0\n",
    "    low = float(sum(table[1:31]))/4096.0\n",
    "    ascii = float(sum(table[32:127]))/4096.0\n",
    "    high = float(sum(table[128:254]))/4096.0\n",
    "    ff = float(table[255])/4096.0\n",
    "    return [zero, low, ascii, high, ff]\n",
    "\n",
    "def get_frequency(filename):\n",
    "    table = [0] * 256\n",
    "    data = open(filename, 'rb')\n",
    "    buff = data.read(2 ** 20)\n",
    "    while buff:\n",
    "        for c in buff:\n",
    "            table[c] += 1\n",
    "        buff = data.read(2 ** 20)\n",
    "    data.close()\n",
    "    table.extend(balanceByte(filename, table))\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path(path_dir):\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "        print('[*]Make dir: {}'.format(path_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(raw_file, data_dir):\n",
    "    if os.path.exists(data_dir):\n",
    "        return\n",
    "    os.makedirs(data_dir)\n",
    "    print('[*]Make dir: {}'.format(data_dir))\n",
    "    chunk_size = 4096\n",
    "    idx = 0\n",
    "    print('[*]Make bin file')\n",
    "    with open(raw_file, \"rb\") as f:\n",
    "        chunk = f.read(chunk_size)\n",
    "        while chunk:\n",
    "            with open(os.path.join(data_dir, str(idx)), \"wb\") as chunk_file:\n",
    "                chunk_file.write(chunk)\n",
    "            idx += 1\n",
    "            chunk = f.read(chunk_size)\n",
    "            print('>>{0:<10}'.format(idx), end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frequency(df, data_dir):\n",
    "    #add frequency col\n",
    "    byte_col = []\n",
    "    for col in range(0, 256):\n",
    "        df[col]=None\n",
    "        byte_col.append(col)\n",
    "    file_info = ['0x00', 'low', 'ascii', 'high','0xff']\n",
    "    for col in file_info:\n",
    "        df[col] = None\n",
    "        byte_col.append(col)\n",
    "    rs = []\n",
    "    for idx in range(0, len(df)):\n",
    "        table = get_frequency(os.path.join(data_dir, df.iloc[idx]['file']))\n",
    "        series = dict(zip(byte_col, table))\n",
    "        rs.append(series)\n",
    "    df.loc[:, byte_col] = pd.DataFrame(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bindata(df, data_dir):\n",
    "    df['data'] = df['file'].map(lambda x: getBinaryData(os.path.join(data_dir, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_type_big(df):\n",
    "    df['type_big'] = df['type'].str.split('-').map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add binary data\n",
    "standard scaling\n",
    "'''\n",
    "def add_col(df, data_dir):\n",
    "    df = df.copy()\n",
    "    df['file'] = df['file'].astype(str)\n",
    "    #add data column\n",
    "    add_frequency(df, data_dir)\n",
    "    add_bindata(df, data_dir)\n",
    "    add_type_big(df)\n",
    "    print('[*]Add col')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(df, scaler):\n",
    "    df = df.copy()\n",
    "    #standard scaler\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != 'object':\n",
    "            scaler.fit(df[[col]])\n",
    "            df[col] = scaler.transform(df[[col]])\n",
    "    print('[*]Apply Scaler')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1D(filter_num,filter_size, dim, num_classes, \n",
    "                  activation='relu', maxpool_size=2):\n",
    "    #cnn input\n",
    "    cnn_input = Input(shape=(dim, 1))\n",
    "    #cnn layer\n",
    "    cnn_layer = Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num)(cnn_input)\n",
    "    cnn_layer = BatchNormalization()(cnn_layer)\n",
    "    cnn_layer = Activation(activation)(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=(maxpool_size))(cnn_layer)\n",
    "    \n",
    "    cnn_layer = Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num)(cnn_layer)\n",
    "    cnn_layer = BatchNormalization()(cnn_layer)\n",
    "    cnn_layer = Activation(activation)(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=(maxpool_size))(cnn_layer)\n",
    "    \n",
    "    cnn_layer = Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num)(cnn_layer)\n",
    "    cnn_layer = BatchNormalization()(cnn_layer)\n",
    "    cnn_layer = Activation(activation)(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=(maxpool_size))(cnn_layer)\n",
    "    cnn_layer = Flatten()(cnn_layer)\n",
    "    \n",
    "    #byte input\n",
    "    byte_input = Input(shape=(261,1), name='byte_input')\n",
    "    #byte layer\n",
    "    byte_layer = Flatten()(byte_input)\n",
    "    \n",
    "    merge_layer = concatenate([cnn_layer, byte_layer])\n",
    "    \n",
    "    merge_layer = Dense(256)(merge_layer)\n",
    "    merge_layer = BatchNormalization()(merge_layer)\n",
    "    merge_layer = Activation(activation)(merge_layer)\n",
    "    \n",
    "    merge_layer = Dense(64)(merge_layer)\n",
    "    merge_layer = BatchNormalization()(merge_layer)\n",
    "    merge_layer = Activation(activation)(merge_layer)\n",
    "    \n",
    "    merge_layer = Dense(num_classes)(merge_layer)\n",
    "    merge_layer = Activation('softmax')(merge_layer)\n",
    "    final_model = Model(inputs=[cnn_input, byte_input], outputs=merge_layer)\n",
    "    \n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data unbalanced -> remove\n",
    "'''\n",
    "def remove_oversample(df, target):\n",
    "    df = df.copy()\n",
    "    min_val = df[target].value_counts().min()\n",
    "    drop_indexes = []\n",
    "    vals = df['type_big'].unique()\n",
    "    for val in vals:\n",
    "        indexes = df[df['type_big']==val].index\n",
    "        if len(indexes) > min_val:\n",
    "            df = df.drop(indexes[min_val:])\n",
    "            drop_indexes.extend(indexes[min_val:])\n",
    "        print('[*]Drop {}: {}->{}'.format(target, len(indexes), min_val))\n",
    "    return df, drop_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "target -> label encoding\n",
    "'''\n",
    "def add_label(df, target):\n",
    "    df = df.copy()\n",
    "    target_label = '{}_label'.format(target)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[target])\n",
    "    df[target_label] = le.transform(df[target])\n",
    "    print('[*]Add col: {}'.format(target_label))\n",
    "    return df, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split x, y by kfold index\n",
    "'''\n",
    "def get_train_data(df, cnn_col, byte_cols, target, num_class, index):\n",
    "    array_dim = len(index)\n",
    "    byte_dim = len(byte_cols)\n",
    "    data_dim =len(df['data'][0])\n",
    "    X_train = np.array([df[cnn_col].iloc[index]]).reshape((array_dim, data_dim, -1))\n",
    "    X_train_byte = np.array([df[byte_cols].iloc[index,:].values]).reshape((array_dim, len(byte_cols), -1))\n",
    "    \n",
    "    Y_train = np.array([df[target].iloc[index]])\n",
    "    Y_train = utils.to_categorical(Y_train, num_classes=num_class).reshape((array_dim, num_class))\n",
    "    return X_train, X_train_byte, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, opt, skf, train_fold_step, byte_cols, cnn_col, target, model_dir):\n",
    "    if cnn_col not in df.columns:\n",
    "        print('[*]{} is not in colummns'.format(cnn_col))\n",
    "    for col in byte_cols:\n",
    "        if col not in df.columns:\n",
    "            print('[*]{} is not in colummns'.format(col))\n",
    "            \n",
    "    histories = []\n",
    "        \n",
    "    for idx1, filter_num in enumerate(params['test_filter_nums']):\n",
    "        for idx2, filter_size in enumerate(params['test_filter_sizes']):\n",
    "            print('==========================filter_num: {} / filter_size: {}=========================='.format(filter_num, filter_size))\n",
    "            #set model name\n",
    "            #save best model of kfold models\n",
    "            model_name = params['model_name_format'].format(filter_num, filter_size)  # save model path\n",
    "            model_path = os.path.join(model_dir, model_name)\n",
    "            \n",
    "            for fold_step, (train_index, valid_index) in enumerate(skf.split(df['type_big'], df['type_big'])):\n",
    "                #num class\n",
    "                num_class = len(df[target].unique())\n",
    "                #cnn dim\n",
    "                cnn_dim = len(df[cnn_col][0])\n",
    "                #set train\n",
    "                X_train, X_train_byte, Y_train = get_train_data(df, cnn_col, byte_cols, target, num_class, train_index)\n",
    "                #set test\n",
    "                X_test, X_test_byte, Y_test = get_train_data(df, cnn_col, byte_cols, target, num_class, valid_index)\n",
    "                #set model\n",
    "                model = CNN1D(filter_num=filter_num, filter_size=filter_size, dim=cnn_dim, num_classes=num_class)\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', f1_m])\n",
    "                \n",
    "                #train\n",
    "                history = model.fit([X_train,X_train_byte], Y_train, \n",
    "                                    batch_size=params['batch_size'],\n",
    "                                    epochs=params['epochs'],\n",
    "                                    validation_data=([X_test, X_test_byte], Y_test),\n",
    "                                    shuffle=True,\n",
    "                                    callbacks=get_callback(model_path, params['lr']),\n",
    "                                    verbose=1)\n",
    "                histories.append(history)\n",
    "\n",
    "                #for train one fold. optional.\n",
    "                if fold_step == train_fold_step-1:\n",
    "                    break\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get history dictionary to dataframe\n",
    "def histories_to_df(histories, target_col):\n",
    "    history_df = pd.DataFrame()\n",
    "    for idx1, filter_num in enumerate(params['test_filter_nums']):\n",
    "        for idx2, filter_size in enumerate(params['test_filter_sizes']):\n",
    "            history = histories[idx1*len(params['test_filter_sizes'])+idx2].history\n",
    "            max_val_idx = history[target_col].index(max(history[target_col]))\n",
    "            temp = {}\n",
    "            for key in history.keys():\n",
    "                temp[key] = history[key][max_val_idx]\n",
    "            history_df['num_{}_size_{}'.format(filter_num, filter_size)] = pd.Series(temp)\n",
    "    return history_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model\n",
    "def get_model_with_weight(select_filter_num, select_filter_size, model_dir, cnn_dim, num_class):\n",
    "    model = CNN1D(filter_num=select_filter_num, filter_size=select_filter_size, dim=cnn_dim, num_classes=num_class)\n",
    "    model_path = params['model_name_format'].format(select_filter_num, select_filter_size)\n",
    "    weight_path = os.path.join(model_dir, model_path)\n",
    "    model.load_weights(weight_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(model, df):\n",
    "    X_test = X_test_byte = Y_test = None\n",
    "    #get test index from k fold\n",
    "    for fold_step, (train_index, valid_index) in enumerate(skf.split(df['type_big_label'], df['type_big_label'])):\n",
    "        X_test, X_test_byte, Y_test = get_train_data(df,\n",
    "                                                 cnn_col='data',\n",
    "                                                 byte_cols=df.columns[4:-2],\n",
    "                                                 target=target_label, \n",
    "                                                 num_class=len(df[target_label].unique()),\n",
    "                                                 index=valid_index)\n",
    "        break\n",
    "    #get y_true\n",
    "    y_true = Y_test\n",
    "    #get y_pred\n",
    "    y_pred = model.predict([X_test, X_test_byte])\n",
    "    # get class name\n",
    "    classes = df[['type_big', 'type_big_label']].head(20)\n",
    "    classes = classes.groupby(['type_big', 'type_big_label'],as_index=False).size()\n",
    "    classes = classes.sort_values().index.levels[0].values\n",
    "    plot_confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1), classes, title='CM')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Set seed: 2\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "input_dir = 'input'\n",
    "#model save\n",
    "model_dir = 'models'\n",
    "#raw & meta csv\n",
    "raw_file = os.path.join(input_dir, 'raw_data.raw')\n",
    "csv_file = os.path.join(input_dir, 'part1.csv')\n",
    "#binary data folder\n",
    "bin_data_dir = os.path.join(input_dir, 'data_bin')\n",
    "df_fix_path = os.path.join(input_dir, 'data_fix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(raw_file):\n",
    "    print(\"[*]raw file isn's exists({})\".format(raw_file))\n",
    "if not os.path.exists(csv_file):\n",
    "    print(\"[*]csv file isn's exists({})\".format(csv_file))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_path(model_dir)\n",
    "make_data(raw_file, bin_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Add col\n",
      "[*]Apply Scaler\n",
      "[*]Drop type_big: 8596->5279\n",
      "[*]Drop type_big: 6764->5279\n",
      "[*]Drop type_big: 5761->5279\n",
      "[*]Drop type_big: 5279->5279\n",
      "[*]Add col: type_big_label\n"
     ]
    }
   ],
   "source": [
    "target = 'type_big'#target or target_big\n",
    "df_fix = None\n",
    "if os.path.exists(df_fix_path):\n",
    "    df_fix = pd.read_csv(df_dix_path)\n",
    "else:\n",
    "    df = pd.read_csv('input/part1.csv')\n",
    "    df = df.drop(['offset'], axis=1)\n",
    "    df.columns = ['file', 'type']\n",
    "    df_fix = add_col(df, bin_data_dir)\n",
    "    df_fix = apply_scaler(df_fix, scaler=StandardScaler())\n",
    "    df_fix, _ = remove_oversample(df_fix, target=target)\n",
    "    df_fix, target_label = add_label(df_fix, target)\n",
    "    df_fix.to_csv(df_fix_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>type</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>0x00</th>\n",
       "      <th>low</th>\n",
       "      <th>ascii</th>\n",
       "      <th>high</th>\n",
       "      <th>0xff</th>\n",
       "      <th>data</th>\n",
       "      <th>type_big</th>\n",
       "      <th>type_big_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>enc-rsa</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00390625</td>\n",
       "      <td>0.11499</td>\n",
       "      <td>0.36084</td>\n",
       "      <td>0.507568</td>\n",
       "      <td>0.00170898</td>\n",
       "      <td>[99, 106, 215, 74, 148, 32, 137, 4, 92, 11, 36...</td>\n",
       "      <td>enc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  file     type   0   1   2   3   4   5   6   7      ...       254 255  \\\n",
       "0    0  enc-rsa  16  10  19  13  22  21  21  16      ...        17   7   \n",
       "\n",
       "         0x00      low    ascii      high        0xff  \\\n",
       "0  0.00390625  0.11499  0.36084  0.507568  0.00170898   \n",
       "\n",
       "                                                data type_big type_big_label  \n",
       "0  [99, 106, 215, 74, 148, 32, 137, 4, 92, 11, 36...      enc              1  \n",
       "\n",
       "[1 rows x 266 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'fold_splits': 10,\n",
    "         'batch_size': 64,\n",
    "         'lr': 0.00001,\n",
    "         'epochs': 2,\n",
    "         'test_filter_nums': [4, 8, 16],\n",
    "         'test_filter_sizes': [3, 4],\n",
    "         'model_name_format': 'model_conv1d_baseline_num_{}_size_{}'}\n",
    "opt = optimizers.Nadam(lr=params['lr'])  # optimizers\n",
    "skf = StratifiedKFold(n_splits=params['fold_splits'], random_state=SEED)  # StaratifiedKFold -> y label 밸런스 유지하며 k fold 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = train(df=df_fix,\n",
    "                  opt=opt, skf=skf, train_fold_step=params['fold_splits'],\n",
    "                  byte_cols=df_fix.columns[4:-2],\n",
    "                  cnn_col='data',\n",
    "                  target=target_label,\n",
    "                  model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = histories_to_df(histories, target_col = 'val_f1_m')\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_filter_num = 8\n",
    "select_filter_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model\n",
    "model = CNN1D(filter_num=select_filter_num, filter_size=select_filter_size, dim=cnn_dim, num_classes=num_class)\n",
    "#load weight\n",
    "model_path = params['model_name_format'].format(select_filter_num, select_filter_size)\n",
    "weight_path = os.path.join(model_dir, model_path)\n",
    "model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "#get test index from train data with kfold\n",
    "plot_cm(test_model, df_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
